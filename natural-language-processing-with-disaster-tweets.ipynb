{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37fbf55e",
   "metadata": {
    "papermill": {
     "duration": 0.006238,
     "end_time": "2024-02-15T18:00:34.647009",
     "exception": false,
     "start_time": "2024-02-15T18:00:34.640771",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Vectorizing Apporach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fb2642f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:34.660047Z",
     "iopub.status.busy": "2024-02-15T18:00:34.659718Z",
     "iopub.status.idle": "2024-02-15T18:00:36.533240Z",
     "shell.execute_reply": "2024-02-15T18:00:36.532457Z"
    },
    "papermill": {
     "duration": 1.882685,
     "end_time": "2024-02-15T18:00:36.535684",
     "exception": false,
     "start_time": "2024-02-15T18:00:34.652999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#### import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# feature_extraction is used for converting text into numerical features \n",
    "from sklearn import feature_extraction\n",
    "# linear_model contains various linear models for regression and classification\n",
    "from sklearn import linear_model\n",
    "# import train_test_split() function module\n",
    "from sklearn import model_selection\n",
    "# load module including utilities for scaling, normalization, ect\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f153a619",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:36.548719Z",
     "iopub.status.busy": "2024-02-15T18:00:36.548218Z",
     "iopub.status.idle": "2024-02-15T18:00:36.617152Z",
     "shell.execute_reply": "2024-02-15T18:00:36.616375Z"
    },
    "papermill": {
     "duration": 0.078084,
     "end_time": "2024-02-15T18:00:36.619634",
     "exception": false,
     "start_time": "2024-02-15T18:00:36.541550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the train and test datasets\n",
    "train_df = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\n",
    "test_df = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1027a211",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:36.632682Z",
     "iopub.status.busy": "2024-02-15T18:00:36.632332Z",
     "iopub.status.idle": "2024-02-15T18:00:36.665126Z",
     "shell.execute_reply": "2024-02-15T18:00:36.664155Z"
    },
    "papermill": {
     "duration": 0.04187,
     "end_time": "2024-02-15T18:00:36.667461",
     "exception": false,
     "start_time": "2024-02-15T18:00:36.625591",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "The concise summary of the train dataset:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n",
      "****************************************************************************************************\n",
      "The shape the train dataset: (7613, 5)\n",
      "****************************************************************************************************\n",
      "The first 5 samples of the dataset:    id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "****************************************************************************************************\n",
      "The text in first 5 samples of the dataset:\n",
      "{0: 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', 1: 'Forest fire near La Ronge Sask. Canada', 2: \"All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\", 3: '13,000 people receive #wildfires evacuation orders in California ', 4: 'Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school '}\n"
     ]
    }
   ],
   "source": [
    "# gain basic structure of the train dataset\n",
    "print('*' * 100)\n",
    "print(f'The concise summary of the train dataset:')\n",
    "train_df.info()\n",
    "\n",
    "print('*' * 100)\n",
    "print(f'The shape the train dataset: {train_df.shape}')\n",
    "\n",
    "print('*' * 100)\n",
    "print(f'The first 5 samples of the dataset: {train_df.head()}')\n",
    "\n",
    "print('*' * 100)\n",
    "print(f'The text in first 5 samples of the dataset:')\n",
    "print({i : train_df.iloc[i]['text'] for i in range(5)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21491ece",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:36.680757Z",
     "iopub.status.busy": "2024-02-15T18:00:36.680443Z",
     "iopub.status.idle": "2024-02-15T18:00:37.378244Z",
     "shell.execute_reply": "2024-02-15T18:00:37.377075Z"
    },
    "papermill": {
     "duration": 0.706899,
     "end_time": "2024-02-15T18:00:37.380558",
     "exception": false,
     "start_time": "2024-02-15T18:00:36.673659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "The vocabularies of tweets: \n",
      "['2fenu1syu6' '2fggzqn1v4' '2fibe2haxc' '2for1lapdances' '2fs649qdwx'\n",
      " '2gljhvead9' '2h0dpmv2ef' '2hocep41kh' '2hours' '2hv2y2m2oz' '2i4eoggo5j'\n",
      " '2iafpmqjep' '2ii3brc7nx' '2jbibeib9g' '2jgvhw7yzs' '2jhtlwuey0'\n",
      " '2jr3yo55dr' '2jxkmkpalp' '2k13' '2k15' '2kdq56xtws' '2lbtshxi3c'\n",
      " '2leezy' '2lgtzkwmqw' '2liwkjybe9' '2lqyxzq5dn' '2m1gneaifl' '2minutemix'\n",
      " '2mnqc73hfk' '2mwc9ywjzy' '2nd' '2nip3d15dx' '2nndbgwyei' '2ns5tfnxpa'\n",
      " '2o7eva1coe' '2okscwyohc' '2oqsgzqlbz' '2oroyunym2' '2pack' '2pcs'\n",
      " '2pimg9bice' '2pm' '2ppzgpxybi' '2q3fuerey5' '2racaivffq' '2rtq9qmgpb'\n",
      " '2sdmichb2z' '2sgdofsmrq' '2slow2report' '2snyghaivs']\n",
      "****************************************************************************************************\n",
      "The shape of the numerical vectors of tweets: \n",
      "(7613, 21637)\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the CountVectorizer()\n",
    "vectorizer = feature_extraction.text.CountVectorizer()\n",
    "\n",
    "# fit the vectorizer to the text data\n",
    "vectorizer.fit(train_df['text'])\n",
    "\n",
    "# print the vocabulary of vectorizer built from the tweets\n",
    "print('*' * 100)\n",
    "print(f'The vocabularies of tweets: \\n{vectorizer.get_feature_names_out()[450:500]}')\n",
    "\n",
    "# transform the text dataset to a sparse matrix of word counts\n",
    "# spare matrix only store non-zero values to save memory\n",
    "X_train = vectorizer.transform(train_df['text'])\n",
    "# using toarray() to convert spare matrix into normal matrix\n",
    "print('*' * 100)\n",
    "print(f'The shape of the numerical vectors of tweets: \\n{X_train.toarray().shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e44d6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:37.394195Z",
     "iopub.status.busy": "2024-02-15T18:00:37.393866Z",
     "iopub.status.idle": "2024-02-15T18:00:37.468073Z",
     "shell.execute_reply": "2024-02-15T18:00:37.467238Z"
    },
    "papermill": {
     "duration": 0.083873,
     "end_time": "2024-02-15T18:00:37.470563",
     "exception": false,
     "start_time": "2024-02-15T18:00:37.386690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transform the text dataset by vectorizer\n",
    "# only fit the vectorizer with train data to prevent data leakage,\n",
    "# which is using information outside the training dataset to create the model\n",
    "X_test = vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1aefc2e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:37.484360Z",
     "iopub.status.busy": "2024-02-15T18:00:37.483987Z",
     "iopub.status.idle": "2024-02-15T18:00:38.042668Z",
     "shell.execute_reply": "2024-02-15T18:00:38.041437Z"
    },
    "papermill": {
     "duration": 0.56956,
     "end_time": "2024-02-15T18:00:38.046433",
     "exception": false,
     "start_time": "2024-02-15T18:00:37.476873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 scores for each fold: [0.59453669 0.5642787  0.64082434]\n"
     ]
    }
   ],
   "source": [
    "# initialize the RidgeClassifier\n",
    "clf = linear_model.RidgeClassifier()\n",
    "\n",
    "# performing cross-validation on a classifier to evaluate its performance of the F1 score\n",
    "# cross-validation provides a more accurate and reliable measure of model performance\n",
    "scores = model_selection.cross_val_score(clf, X_train, train_df[\"target\"], cv=3, scoring=\"f1\")\n",
    "print(f'F1 scores for each fold: {scores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "355519db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:38.075157Z",
     "iopub.status.busy": "2024-02-15T18:00:38.074606Z",
     "iopub.status.idle": "2024-02-15T18:00:38.364261Z",
     "shell.execute_reply": "2024-02-15T18:00:38.362845Z"
    },
    "papermill": {
     "duration": 0.30893,
     "end_time": "2024-02-15T18:00:38.368973",
     "exception": false,
     "start_time": "2024-02-15T18:00:38.060043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the classifier on the training dataset\n",
    "clf.fit(X_train, train_df[\"target\"])\n",
    "# save the prediction\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = clf.predict(X_test)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7818470d",
   "metadata": {
    "papermill": {
     "duration": 0.013024,
     "end_time": "2024-02-15T18:00:38.396523",
     "exception": false,
     "start_time": "2024-02-15T18:00:38.383499",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pretrained DistilBERT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a63e5559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:00:38.424780Z",
     "iopub.status.busy": "2024-02-15T18:00:38.424181Z",
     "iopub.status.idle": "2024-02-15T18:01:04.925800Z",
     "shell.execute_reply": "2024-02-15T18:01:04.924407Z"
    },
    "papermill": {
     "duration": 26.518725,
     "end_time": "2024-02-15T18:01:04.928428",
     "exception": false,
     "start_time": "2024-02-15T18:00:38.409703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-core in /opt/conda/lib/python3.10/site-packages (0.1.7)\r\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.4.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-core) (1.24.4)\r\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-core) (13.7.0)\r\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.0.7)\r\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core) (3.10.0)\r\n",
      "Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core) (0.1.8)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-core) (2.17.2)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-core) (0.1.2)\r\n"
     ]
    }
   ],
   "source": [
    "# pgrade the keras-core package to the latest version\n",
    "!pip install keras-core --upgrade\n",
    "# upgrade the keras-nlp package to the latest version with minimal output during the installation process\n",
    "!pip install -q keras-nlp --upgrade\n",
    "import os\n",
    "# set the environment variable KERAS_BACKEND to 'tensorflow' to specify TensorFlow as the backend for Keras\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3882c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:04.943076Z",
     "iopub.status.busy": "2024-02-15T18:01:04.942692Z",
     "iopub.status.idle": "2024-02-15T18:01:19.390764Z",
     "shell.execute_reply": "2024-02-15T18:01:19.389678Z"
    },
    "papermill": {
     "duration": 14.45805,
     "end_time": "2024-02-15T18:01:19.393071",
     "exception": false,
     "start_time": "2024-02-15T18:01:04.935021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-15 18:01:06.834620: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-15 18:01:06.834725: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-15 18:01:06.971666: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n",
      "TensorFlow version: 2.15.0\n",
      "KerasNLP version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow is an open-source library for numerical computation and machine learning\n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "# keras_nlp is a library providing Natural Language Processing (NLP) tools and models for use with Keras\n",
    "import keras_nlp\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# these are used to visualize and calculate a confusion matrix, \n",
    "# which is a table used to describe the performance of a classification model.\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# seaborn is a visualization tool based on matplotlib\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98dcfbab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:19.409093Z",
     "iopub.status.busy": "2024-02-15T18:01:19.408507Z",
     "iopub.status.idle": "2024-02-15T18:01:19.431390Z",
     "shell.execute_reply": "2024-02-15T18:01:19.430338Z"
    },
    "papermill": {
     "duration": 0.032496,
     "end_time": "2024-02-15T18:01:19.433279",
     "exception": false,
     "start_time": "2024-02-15T18:01:19.400783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "Train Length Stat\n",
      "count    7613.000000\n",
      "mean      101.037436\n",
      "std        33.781325\n",
      "min         7.000000\n",
      "25%        78.000000\n",
      "50%       107.000000\n",
      "75%       133.000000\n",
      "max       157.000000\n",
      "Name: length, dtype: float64\n",
      "****************************************************************************************************\n",
      "Test Length Stat\n",
      "count    3263.000000\n",
      "mean      102.108183\n",
      "std        33.972158\n",
      "min         5.000000\n",
      "25%        78.000000\n",
      "50%       109.000000\n",
      "75%       134.000000\n",
      "max       151.000000\n",
      "Name: length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# explore the statistic information\n",
    "train_df[\"length\"] = train_df[\"text\"].apply(lambda x : len(x))\n",
    "test_df[\"length\"] = test_df[\"text\"].apply(lambda x : len(x))\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"Train Length Stat\")\n",
    "print(train_df[\"length\"].describe())\n",
    "\n",
    "print('*' * 100)\n",
    "print(\"Test Length Stat\")\n",
    "print(test_df[\"length\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c51102be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:19.447732Z",
     "iopub.status.busy": "2024-02-15T18:01:19.447421Z",
     "iopub.status.idle": "2024-02-15T18:01:19.452361Z",
     "shell.execute_reply": "2024-02-15T18:01:19.451510Z"
    },
    "papermill": {
     "duration": 0.014313,
     "end_time": "2024-02-15T18:01:19.454167",
     "exception": false,
     "start_time": "2024-02-15T18:01:19.439854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAINING_EXAMPLES = train_df.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "# number of batches of data\n",
    "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 5\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "462bbf16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:19.468644Z",
     "iopub.status.busy": "2024-02-15T18:01:19.468352Z",
     "iopub.status.idle": "2024-02-15T18:01:19.476085Z",
     "shell.execute_reply": "2024-02-15T18:01:19.475400Z"
    },
    "papermill": {
     "duration": 0.017455,
     "end_time": "2024-02-15T18:01:19.478162",
     "exception": false,
     "start_time": "2024-02-15T18:01:19.460707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data\n",
    "X = train_df[\"text\"]\n",
    "y = train_df[\"target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "X_test = test_df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de863a62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:19.493483Z",
     "iopub.status.busy": "2024-02-15T18:01:19.493132Z",
     "iopub.status.idle": "2024-02-15T18:01:30.438694Z",
     "shell.execute_reply": "2024-02-15T18:01:30.437802Z"
    },
    "papermill": {
     "duration": 10.954764,
     "end_time": "2024-02-15T18:01:30.440602",
     "exception": false,
     "start_time": "2024-02-15T18:01:19.485838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "Attaching 'tokenizer.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "Attaching 'assets/tokenizer/vocabulary.txt' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "Attaching 'config.json' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "Attaching 'model.weights.h5' from model 'keras/distil_bert/keras/distil_bert_base_en_uncased/2' to your Kaggle notebook...\n",
      "/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  return id(getattr(self, attr)) not in self._functional_layer_ids\n",
      "/opt/conda/lib/python3.10/site-packages/keras_nlp/src/models/backbone.py:37: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  return id(getattr(self, attr)) not in self._functional_layer_ids\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"preprocessor_4_tweets\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"preprocessor_4_tweets\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ distil_bert_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DistilBertTokenizer</span>)        │                                              <span style=\"color: #00af00; text-decoration-color: #00af00\">30,522</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ distil_bert_tokenizer (\u001b[38;5;33mDistilBertTokenizer\u001b[0m)        │                                              \u001b[38;5;34m30,522\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"distil_bert_classifier\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"distil_bert_classifier\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                  </span>┃<span style=\"font-weight: bold\"> Output Shape                           </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ distil_bert_backbone (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DistilBertBackbone</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                      │      <span style=\"color: #00af00; text-decoration-color: #00af00\">66,362,880</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ tf.__operators__.getitem (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SlicingOpLambda</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ pooled_dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">590,592</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ classifier_dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ logits (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,538</span> │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape                          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ distil_bert_backbone (\u001b[38;5;33mDistilBertBackbone\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                      │      \u001b[38;5;34m66,362,880\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ tf.__operators__.getitem (\u001b[38;5;33mSlicingOpLambda\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ pooled_dense (\u001b[38;5;33mDense\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                            │         \u001b[38;5;34m590,592\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ classifier_dropout (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                            │               \u001b[38;5;34m0\u001b[0m │\n",
       "├───────────────────────────────────────────────┼────────────────────────────────────────┼─────────────────┤\n",
       "│ logits (\u001b[38;5;33mDense\u001b[0m)                                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                              │           \u001b[38;5;34m1,538\u001b[0m │\n",
       "└───────────────────────────────────────────────┴────────────────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,955,010</span> (255.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m66,955,010\u001b[0m (255.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">66,955,010</span> (255.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m66,955,010\u001b[0m (255.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a DistilBERT model path\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "# initialize the preprocessor from pre-trained model\n",
    "# preprocessor includes steps like tokenization, convert tokens to IDs and creating attention mask\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "                                                                   sequence_length=160,\n",
    "                                                                   name=\"preprocessor_4_tweets\"\n",
    "                                                                  )\n",
    "\n",
    "# load pretrained classifier model\n",
    "# the classifier is meant to take the preprocessed input data and perform classification tasks,\n",
    "# outputting the probability that the input text belongs to one of the specified classes\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor, \n",
    "                                                               num_classes=2)\n",
    "# display the classifier summary\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68729209",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:01:30.458972Z",
     "iopub.status.busy": "2024-02-15T18:01:30.458403Z",
     "iopub.status.idle": "2024-02-15T18:07:26.060066Z",
     "shell.execute_reply": "2024-02-15T18:07:26.059012Z"
    },
    "papermill": {
     "duration": 355.65025,
     "end_time": "2024-02-15T18:07:26.099397",
     "exception": false,
     "start_time": "2024-02-15T18:01:30.449147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708020111.618369      76 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 93s 360ms/step - loss: 0.4833 - accuracy: 0.7898 - val_loss: 0.3900 - val_accuracy: 0.8450\n",
      "Epoch 2/5\n",
      "191/191 [==============================] - 64s 336ms/step - loss: 0.3651 - accuracy: 0.8540 - val_loss: 0.3821 - val_accuracy: 0.8444\n",
      "Epoch 3/5\n",
      "191/191 [==============================] - 64s 335ms/step - loss: 0.3203 - accuracy: 0.8760 - val_loss: 0.3868 - val_accuracy: 0.8372\n",
      "Epoch 4/5\n",
      "191/191 [==============================] - 64s 338ms/step - loss: 0.2706 - accuracy: 0.9010 - val_loss: 0.4344 - val_accuracy: 0.8280\n",
      "Epoch 5/5\n",
      "191/191 [==============================] - 64s 336ms/step - loss: 0.2308 - accuracy: 0.9204 - val_loss: 0.4717 - val_accuracy: 0.8240\n"
     ]
    }
   ],
   "source": [
    "# compile the classifier model with desired settings\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), # classes are represented as integers.'from_logits=True' and the outputs of the model are logits and not probabilities\n",
    "    optimizer=Adam(learning_rate=1e-5), # Adam optimizer with a very low learning rate (1e-5) for NN\n",
    "    metrics=[\"accuracy\"] # monitor the \"accuracy\" metric during training to see the percentage of correctly classified instances in each epoch.\n",
    ")\n",
    "\n",
    "# train the model on the given dataset\n",
    "history = classifier.fit(\n",
    "    x=X_train, # training features\n",
    "    y=y_train, # target data\n",
    "    batch_size=BATCH_SIZE, # the number of samples per gradient update\n",
    "    epochs=EPOCHS, # the times go over entire dataset\n",
    "    validation_data=(X_val, y_val) # the dataset to evaluate model metrics like loss and accuracy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56312553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:07:26.273490Z",
     "iopub.status.busy": "2024-02-15T18:07:26.272616Z",
     "iopub.status.idle": "2024-02-15T18:07:26.278910Z",
     "shell.execute_reply": "2024-02-15T18:07:26.278013Z"
    },
    "papermill": {
     "duration": 0.095545,
     "end_time": "2024-02-15T18:07:26.280890",
     "exception": false,
     "start_time": "2024-02-15T18:07:26.185345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def displayConfusionMatrix(true_labels, predicted_labels, title):\n",
    "    # compute the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "    \n",
    "    # create a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix: {title}')\n",
    "    plt.ylabel('Actual Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout() \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a151750a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:07:26.452181Z",
     "iopub.status.busy": "2024-02-15T18:07:26.451372Z",
     "iopub.status.idle": "2024-02-15T18:07:55.197682Z",
     "shell.execute_reply": "2024-02-15T18:07:55.196775Z"
    },
    "papermill": {
     "duration": 28.834501,
     "end_time": "2024-02-15T18:07:55.199994",
     "exception": false,
     "start_time": "2024-02-15T18:07:26.365493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191/191 [==============================] - 28s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "# make the prediction\n",
    "y_pred_train = classifier.predict(X_train)\n",
    "\n",
    "# call the function with your labels\n",
    "# displayConfusionMatrix(y_train, y_pred_train, \"Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14419715",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-15T18:07:55.429941Z",
     "iopub.status.busy": "2024-02-15T18:07:55.429521Z",
     "iopub.status.idle": "2024-02-15T18:08:10.966304Z",
     "shell.execute_reply": "2024-02-15T18:08:10.965078Z"
    },
    "papermill": {
     "duration": 15.656527,
     "end_time": "2024-02-15T18:08:10.968633",
     "exception": false,
     "start_time": "2024-02-15T18:07:55.312106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 15s 137ms/step\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "# save the prediction\n",
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "print('Completed!')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 869809,
     "sourceId": 17777,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 7429247,
     "modelInstanceId": 4689,
     "sourceId": 6068,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30648,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 462.382399,
   "end_time": "2024-02-15T18:08:14.272313",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-15T18:00:31.889914",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
